''Using Python language do the exploratory data analysis of dataset 
imported in the lab 4.''

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Display settings
pd.set_option('display.max_columns', None)  # So I can see all columns in DataFrame outputs
sns.set(style="whitegrid")

# Step 1: Load the dataset (I'll replace 'lab4_dataset.csv' with the actual dataset path)
data = pd.read_csv('eda_sample_dataset.csv')

# Step 2: Check the structure of the dataset
print(f"Shape of dataset: {data.shape}")  # Rows and columns
print("\nColumns and Data Types:")
print(data.dtypes)  # Data types of each column

# Step 3: View missing values
print("\nMissing values in each column:")
print(data.isnull().sum())

# Step 4: Basic statistical summary for numerical columns
print("\nSummary statistics for numerical columns:")
print(data.describe())

# Step 5: View first few rows of the dataset
print("\nFirst 5 rows of the dataset:")
print(data.head())

# Step 6: Exploratory Data Analysis

# 6.1 Checking for duplicate rows
print(f"\nNumber of duplicate rows: {data.duplicated().sum()}")

# 6.2 Handling missing data
# I'll drop columns with a significant percentage of missing values (threshold 50% missing)
missing_threshold = 0.5
missing_percentage = data.isnull().mean()
columns_to_drop = missing_percentage[missing_percentage > missing_threshold].index
print(f"\nColumns to drop due to >{missing_threshold*100}% missing values: {list(columns_to_drop)}")
data = data.drop(columns=columns_to_drop)

# For remaining missing values, I'll fill with the median (or mean) of the column
for col in data.columns:
    if data[col].isnull().sum() > 0:
        if data[col].dtype in ['float64', 'int64']:
            data[col].fillna(data[col].median(), inplace=True)  # For numerical columns
        else:
            data[col].fillna(data[col].mode()[0], inplace=True)  # For categorical columns

# Step 7: Advanced EDA Visualization

# 7.1 Advanced Missing Data Visualization using Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title("Heatmap of Missing Data")
plt.show()

# 7.2 Visualize the distribution of numerical columns
print("\nVisualizing distribution of numerical columns...")
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[col], kde=True, bins=30, color='skyblue')
    plt.title(f'Distribution of {col}')
    plt.show()

# 7.3 Visualizing categorical columns (if any)
print("\nVisualizing distribution of categorical columns...")
for col in data.select_dtypes(include=['object']).columns:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=col, data=data, palette='Set2')
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45)
    plt.show()

# 7.4 Calculating Skewness and Kurtosis for Numerical Columns
print("\nSkewness and Kurtosis of Numerical Columns:")
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    skewness = data[col].skew()
    kurtosis = data[col].kurt()
    print(f"{col}: Skewness = {skewness:.2f}, Kurtosis = {kurtosis:.2f}")

# 7.5 Outlier detection using boxplots
print("\nVisualizing outliers using boxplots...")
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure(figsize=(8, 4))
    sns.boxplot(data[col], color='lightgreen')
    plt.title(f'Boxplot of {col}')
    plt.show()

# 7.6 Detect and remove outliers using IQR (Interquartile Range)
print("\nRemoving outliers using IQR...")
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    # Filter out outliers
    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

print(f"\nDataset shape after outlier removal: {data.shape}")

# Step 8: Correlation analysis

# 8.1 Correlation matrix
# I'll encode categorical columns using one-hot encoding
data_encoded = pd.get_dummies(data, drop_first=True)

# Now I'll calculate the correlation matrix
print("\nCorrelation Matrix (after encoding):")
corr_matrix = data_encoded.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.show()

# 8.2 Analyzing the correlation between features and the target variable
# I assume 'target' is the column name for the target variable (replace with actual target name)
if 'target' in data.columns:
    target_corr = corr_matrix['target'].sort_values(ascending=False)
    print("\nFeature Correlations with Target Variable:")
    print(target_corr)

# Step 9: Checking Multicollinearity using Variance Inflation Factor (VIF)
print("\nChecking Multicollinearity using VIF:")
X = data.select_dtypes(include=['float64', 'int64'])  # Selecting only numerical columns
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_data)

# Step 10: Pairplot for numerical features (to visualize pairwise relationships)
sample_size = min(100, len(data))  # I'll take 100 or fewer rows if the dataset is smaller
sns.pairplot(data.sample(sample_size), diag_kind='kde', corner=True)
plt.show()

# Step 11: Feature Engineering Suggestions

# 11.1 Encoding categorical variables
for col in data.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# 11.2 Scaling numerical features (optional but useful for ML models)
scaler = StandardScaler()
data_scaled = pd.DataFrame(scaler.fit_transform(data.select_dtypes(include=['float64', 'int64'])), columns=data.select_dtypes(include=['float64', 'int64']).columns)

# Step 12: Saving cleaned and preprocessed dataset
data.to_csv('cleaned_lab4_dataset.csv', index=False)
